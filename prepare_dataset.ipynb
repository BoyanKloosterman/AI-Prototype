{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26031b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA CLEANING & ENRICHMENT PROCESS\n",
      "============================================================\n",
      "Origineel: 211 rijen, 20 kolommen\n",
      "\n",
      "1. Kolommen geselecteerd: 16 van 16\n",
      "   Behouden: ['id', 'name', 'shortdescription', 'description', 'content', 'studycredit', 'location', 'contact_id', 'level', 'learningoutcomes', 'module_tags', 'interests_match_score', 'popularity_score', 'estimated_difficulty', 'available_spots', 'start_date']\n",
      "\n",
      "2. Vervangen van 'ntb' waarden\n",
      "   shortdescription: 10 'ntb' waarden vervangen\n",
      "   description: 2 'ntb' waarden vervangen\n",
      "   content: 2 'ntb' waarden vervangen\n",
      "   learningoutcomes: 26 'ntb' waarden vervangen\n",
      "\n",
      "2b. Auto-genereren van module tags voor modules zonder tags\n",
      "   30 modules zonder tags → 0 na auto-generatie\n",
      "\n",
      "3. Vullen van lege content velden\n",
      "   shortdescription: 20 lege velden → 0 na vullen\n",
      "   description: 0 lege velden → 0 na vullen\n",
      "   content: 0 lege velden → 0 na vullen\n",
      "\n",
      "4. Genereren van learning outcomes\n",
      "   36 ontbrekende learning outcomes → 0 na vullen\n",
      "\n",
      "5. start_date geconverteerd naar datetime\n",
      "   Ongeldige datums naar NaT: 0\n",
      "\n",
      "6. Duplicaten op 'id'\n",
      "   Gevonden: 0, Rijen voor: 211, na: 211\n",
      "\n",
      "7. Tekstvelden normaliseren en lemmatiseren\n",
      "   - Verwerken: shortdescription -> shortdescription_clean\n",
      "   - Verwerken: description -> description_clean\n",
      "   - Verwerken: content -> content_clean\n",
      "   - Verwerken: learningoutcomes -> learningoutcomes_clean\n",
      "\n",
      "============================================================\n",
      "FINALE DATASET STATUS\n",
      "============================================================\n",
      "Rijen: 211\n",
      "Kolommen: 20\n",
      "Totaal NULL waarden: 0\n",
      "\n",
      "NULL waarden per kolom (alleen > 0):\n",
      "   Geen NULL waarden gevonden! ✓\n",
      "\n",
      "============================================================\n",
      "DATA QUALITY VERIFICATIE\n",
      "============================================================\n",
      "✓ Geen 'ntb' waarden meer aanwezig\n",
      "✓ Alle kritieke tekstvelden zijn gevuld\n",
      "\n",
      "Module tags: 211/211 modules hebben tags\n",
      "\n",
      "Sample (eerste 3 rijen):\n",
      "    id                          name                                   shortdescription                                   learningoutcomes\n",
      "0  159  Kennismaking met Psychologie  Brein, gedragsbeinvloeding, ontwikkelingspsych...  A. Je beantwoordt vragen in een meerkeuze kenn...\n",
      "1  160   Learning and working abroad  Internationaal, persoonlijke ontwikkeling, ver...  De student toont professioneel gedrag conform ...\n",
      "2  161       Proactieve zorgplanning     Proactieve zorgplanning, cocreatie, ziekenhuis  De student past pro actieve zorgplanning toe b...\n",
      "\n",
      "============================================================\n",
      "✓ OPGESLAGEN: Uitgebreide_VKM_dataset_cleaned.csv\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kloos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kloos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kloos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\kloos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "INPUT_FILE = \"Uitgebreide_VKM_dataset.csv\"\n",
    "OUTPUT_FILE = \"Uitgebreide_VKM_dataset_cleaned.csv\"\n",
    "\n",
    "# Kolommen die we willen behouden in de finale dataset\n",
    "KEEP_COLUMNS = [\n",
    "    \"id\", \"name\", \"shortdescription\", \"description\", \"content\",\n",
    "    \"studycredit\", \"location\", \"contact_id\", \"level\", \"learningoutcomes\",\n",
    "    \"module_tags\", \"interests_match_score\", \"popularity_score\",\n",
    "    \"estimated_difficulty\", \"available_spots\", \"start_date\"\n",
    "]\n",
    "\n",
    "# Kolommen die we als tekst willen schoonmaken\n",
    "TEXT_COLS = [\n",
    "    \"shortdescription\",\n",
    "    \"description\",\n",
    "    \"content\",\n",
    "    \"learningoutcomes\"\n",
    "]\n",
    "\n",
    "# Stopwoorden en lemmatizer\n",
    "# Kies 'english' of 'dutch' afhankelijk van je dataset\n",
    "STOP_LANG = \"dutch\"   # of \"english\"\n",
    "stop_words = set(stopwords.words(STOP_LANG))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_length(tokens, max_len=200):\n",
    "    \"\"\"Beperk tokens tot een vaste maximale lengte om scheefheid te verminderen.\"\"\"\n",
    "    return tokens[:max_len]\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Maak tekst schoon, lemmatiseer en normaliseer lengte.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Verwijder alles behalve letters, cijfers en spaties\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\sáéíóúàèìòùäëïöüâêîôûçñ]\", \" \", text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Filter op stopwoorden en korte tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "\n",
    "    # Lemmatiseer\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "    tokens = normalize_length(tokens, max_len=200)\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def fill_missing_content(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Vul lege shortdescription/description/content met intelligente fallbacks.\"\"\"\n",
    "    \n",
    "    # 1. shortdescription: gebruik description[:200] of name\n",
    "    mask = df['shortdescription'].isna() | (df['shortdescription'].str.strip() == '') | (df['shortdescription'].str.lower() == 'ntb')\n",
    "    df.loc[mask, 'shortdescription'] = df.loc[mask].apply(\n",
    "        lambda row: row['description'][:200] if pd.notna(row['description']) and row['description'].strip() and row['description'].lower() != 'ntb'\n",
    "        else f\"{row['name']} - Een verdiepende module voor verdere professionele ontwikkeling.\",\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # 2. description: gebruik content of shortdescription + name\n",
    "    mask = df['description'].isna() | (df['description'].str.strip() == '') | (df['description'].str.lower() == 'ntb')\n",
    "    df.loc[mask, 'description'] = df.loc[mask].apply(\n",
    "        lambda row: row['content'] if pd.notna(row['content']) and row['content'].strip() and row['content'].lower() != 'ntb'\n",
    "        else f\"In de module {row['name']} verdiep je je in {row['shortdescription'] if pd.notna(row['shortdescription']) else 'relevante onderwerpen'}. Deze module biedt praktijkgerichte kennis en vaardigheden die je voorbereidt op professioneel werken in het vakgebied.\",\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # 3. content: gebruik description\n",
    "    mask = df['content'].isna() | (df['content'].str.strip() == '') | (df['content'].str.lower() == 'ntb')\n",
    "    df.loc[mask, 'content'] = df.loc[mask, 'description']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_learning_outcomes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Vul learningoutcomes met passende waarden op basis van context.\"\"\"\n",
    "    \n",
    "    # Generieke learning outcomes per niveau\n",
    "    nlqf5_default = \"Na afronding van deze module beschik je over de kennis en vaardigheden om professioneel te handelen binnen dit vakgebied. Je kunt theoretische kennis toepassen in praktijksituaties en reflecteert op je eigen handelen.\"\n",
    "    \n",
    "    nlqf6_default = \"Na afronding van deze module kun je complexe vraagstukken analyseren en oplossen binnen dit vakgebied. Je werkt methodisch, reflecteert kritisch op je handelen en draagt bij aan innovatie en kennisontwikkeling in de professionele praktijk.\"\n",
    "    \n",
    "    # Specifieke learning outcomes op basis van module_tags\n",
    "    tag_based_outcomes = {\n",
    "        'technologie': \"Je past technologische innovaties toe in de praktijk en evalueert hun impact op het vakgebied.\",\n",
    "        'zorg': \"Je levert zorg op professioneel niveau, afgestemd op de behoeften van de zorgvrager en in lijn met beroepscodes.\",\n",
    "        'welzijn': \"Je draagt bij aan het welzijn van individuen en groepen door passende interventies en ondersteuning te bieden.\",\n",
    "        'ondernemen': \"Je ontwikkelt ondernemende vaardigheden en kunt zakelijke kansen identificeren en realiseren.\",\n",
    "        'internationaal': \"Je werkt in internationale context en toont interculturele competenties.\",\n",
    "        'jeugd': \"Je werkt methodisch met jeugdigen en hun systemen, rekening houdend met ontwikkelingspsychologische aspecten.\",\n",
    "        'ontwerp': \"Je ontwikkelt en evalueert ontwerpen die voldoen aan gebruikerseisen en professionele standaarden.\",\n",
    "        'onderzoek': \"Je voert praktijkgericht onderzoek uit en vertaalt bevindingen naar concrete aanbevelingen.\",\n",
    "    }\n",
    "    \n",
    "    def generate_learning_outcome(row):\n",
    "        if pd.notna(row['learningoutcomes']) and row['learningoutcomes'].strip() and row['learningoutcomes'].lower() not in ['ntb', 'nader te bepalen', 'nog niet bepaald']:\n",
    "            return row['learningoutcomes']\n",
    "        \n",
    "        # Basis outcome op basis van niveau\n",
    "        level = row.get('level', 'NLQF5')\n",
    "        base_outcome = nlqf6_default if 'NLQF6' in str(level) else nlqf5_default\n",
    "        \n",
    "        # Probeer tag-based outcome toe te voegen\n",
    "        tags = row.get('module_tags', [])\n",
    "        if isinstance(tags, str):\n",
    "            tags = tags.lower().split(',')\n",
    "        elif isinstance(tags, list):\n",
    "            tags = [str(t).lower() for t in tags]\n",
    "        \n",
    "        specific_outcome = \"\"\n",
    "        for tag in tags:\n",
    "            tag_clean = tag.strip().strip(\"[]'\\\"\")\n",
    "            for key, outcome in tag_based_outcomes.items():\n",
    "                if key in tag_clean:\n",
    "                    specific_outcome = f\" {outcome}\"\n",
    "                    break\n",
    "            if specific_outcome:\n",
    "                break\n",
    "        \n",
    "        return base_outcome + specific_outcome\n",
    "    \n",
    "    df['learningoutcomes'] = df.apply(generate_learning_outcome, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_module_tags(tag_value):\n",
    "    \"\"\"Parse module_tags van string naar lijst.\"\"\"\n",
    "    import ast\n",
    "    \n",
    "    if pd.isna(tag_value):\n",
    "        return []\n",
    "    \n",
    "    # Als het al een lijst is\n",
    "    if isinstance(tag_value, list):\n",
    "        return tag_value\n",
    "    \n",
    "    # Converteer naar string en clean\n",
    "    tag_str = str(tag_value).strip()\n",
    "    \n",
    "    # Lege string\n",
    "    if not tag_str or tag_str == '':\n",
    "        return []\n",
    "    \n",
    "    # Check voor 'ntb' varianten\n",
    "    if tag_str.lower() in ['ntb', '[]', \"['ntb']\", '[\"ntb\"]']:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Probeer als Python literal te evalueren\n",
    "        parsed = ast.literal_eval(tag_str)\n",
    "        if isinstance(parsed, list):\n",
    "            # Filter 'ntb' tags eruit\n",
    "            return [tag for tag in parsed if str(tag).lower() != 'ntb']\n",
    "        return []\n",
    "    except (ValueError, SyntaxError):\n",
    "        # Als het geen geldige Python literal is, split op komma's\n",
    "        if ',' in tag_str:\n",
    "            tags = [t.strip().strip(\"[]'\\\"\") for t in tag_str.split(',')]\n",
    "            return [tag for tag in tags if tag and tag.lower() != 'ntb']\n",
    "        else:\n",
    "            # Enkele tag\n",
    "            cleaned = tag_str.strip(\"[]'\\\"\")\n",
    "            if cleaned and cleaned.lower() != 'ntb':\n",
    "                return [cleaned]\n",
    "            return []\n",
    "\n",
    "\n",
    "def generate_tags_from_text(row):\n",
    "    \"\"\"Genereer automatisch tags op basis van module naam en beschrijving.\"\"\"\n",
    "    \n",
    "    # Verzamel tekst voor analyse\n",
    "    text_parts = []\n",
    "    if pd.notna(row.get('name')):\n",
    "        text_parts.append(str(row['name']).lower())\n",
    "    if pd.notna(row.get('shortdescription')):\n",
    "        text_parts.append(str(row['shortdescription']).lower())\n",
    "    \n",
    "    combined_text = ' '.join(text_parts)\n",
    "    \n",
    "    # Woordenlijst voor tag detectie (meest relevante keywords)\n",
    "    tag_keywords = {\n",
    "        'technologie': ['technologie', 'digitalisering', 'digitaal', 'ict', 'software', 'data', 'ai', 'robotica', 'bim', 'virtual reality'],\n",
    "        'zorg': ['zorg', 'verpleegkunde', 'verpleegkundige', 'patient', 'gezondheidszorg', 'medisch', 'behandeling', 'therapie'],\n",
    "        'welzijn': ['welzijn', 'welzijnswerk', 'maatschappelijk', 'sociaal werk', 'participatie', 'inclusie'],\n",
    "        'psychologie': ['psychologie', 'psychisch', 'gedrag', 'mentaal', 'cognitief', 'emotioneel'],\n",
    "        'onderwijs': ['onderwijs', 'pedagogisch', 'didactisch', 'lesgeven', 'docent', 'leren', 'onderwijzen'],\n",
    "        'management': ['management', 'organisatie', 'leiderschap', 'projectmanagement', 'bedrijfskunde'],\n",
    "        'innovatie': ['innovatie', 'innovatief', 'vernieuwing', 'ontwikkeling', 'design thinking'],\n",
    "        'internationaal': ['internationaal', 'buitenland', 'intercultureel', 'global', 'abroad'],\n",
    "        'jeugd': ['jeugd', 'kinderen', 'jongeren', 'kind', 'baby', 'adolescent'],\n",
    "        'ontwerp': ['ontwerp', 'design', 'creatie', 'vormgeving', 'architectuur', 'bouwen'],\n",
    "        'onderzoek': ['onderzoek', 'research', 'analyse', 'wetenschappelijk'],\n",
    "        'duurzaamheid': ['duurzaam', 'duurzaamheid', 'circulair', 'groen', 'klimaat', 'milieu', 'biobased'],\n",
    "        'ondernemen': ['ondernemen', 'ondernemerschap', 'business', 'startup', 'zzp'],\n",
    "        'communicatie': ['communicatie', 'gesprek', 'presenteren', 'storytelling'],\n",
    "        'gezondheid': ['gezondheid', 'preventie', 'leefstijl', 'beweging', 'voeding'],\n",
    "    }\n",
    "    \n",
    "    # Detecteer relevante tags\n",
    "    detected_tags = []\n",
    "    for tag, keywords in tag_keywords.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in combined_text:\n",
    "                detected_tags.append(tag)\n",
    "                break  # Één match per tag is genoeg\n",
    "    \n",
    "    # Als er geen tags gedetecteerd zijn, maak generieke tags op basis van woorden\n",
    "    if not detected_tags:\n",
    "        # Haal belangrijke woorden uit de naam\n",
    "        words = str(row.get('name', '')).lower().split()\n",
    "        # Filter stopwoorden en korte woorden\n",
    "        important_words = [w for w in words if len(w) > 4 and w not in ['deze', 'voor', 'binnen', 'module', 'minor']]\n",
    "        detected_tags = important_words[:3]  # Max 3 woorden als fallback\n",
    "    \n",
    "    return detected_tags[:5]  # Max 5 tags per module\n",
    "\n",
    "\n",
    "def replace_ntb_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Vervang alle 'ntb' of 'Ntb' waarden met 'Nog niet bepaald'.\"\"\"\n",
    "    \n",
    "    # Definieer kolommen waar we ntb willen vervangen (tekstuele kolommen)\n",
    "    text_columns = ['shortdescription', 'description', 'content', 'learningoutcomes', 'name']\n",
    "    \n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            # Vervang ntb/Ntb/NTB met \"Nog niet bepaald\"\n",
    "            mask = df[col].str.lower().str.strip() == 'ntb'\n",
    "            df.loc[mask, col] = 'Nog niet bepaald'\n",
    "    \n",
    "    # Voor module_tags: parse van string naar lijst\n",
    "    if 'module_tags' in df.columns:\n",
    "        df['module_tags'] = df['module_tags'].apply(parse_module_tags)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_empty_tags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Vul lege module_tags met automatisch gegenereerde tags.\"\"\"\n",
    "    \n",
    "    if 'module_tags' not in df.columns:\n",
    "        return df\n",
    "    \n",
    "    # Vind modules zonder tags\n",
    "    empty_mask = df['module_tags'].apply(lambda x: len(x) == 0 if isinstance(x, list) else True)\n",
    "    \n",
    "    # Genereer tags voor modules zonder tags\n",
    "    df.loc[empty_mask, 'module_tags'] = df[empty_mask].apply(generate_tags_from_text, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def clean_vkm_dataset(input_file: str, output_file: str) -> pd.DataFrame:\n",
    "    # Data inladen\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA CLEANING & ENRICHMENT PROCESS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Origineel: {df.shape[0]} rijen, {df.shape[1]} kolommen\")\n",
    "\n",
    "    df_cleaned = df.copy()\n",
    "\n",
    "    # 1. Selecteer alleen de gewenste kolommen\n",
    "    available_cols = [c for c in KEEP_COLUMNS if c in df_cleaned.columns]\n",
    "    missing_cols = [c for c in KEEP_COLUMNS if c not in df_cleaned.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\nVolgende kolommen ontbreken: {missing_cols}\")\n",
    "    \n",
    "    df_cleaned = df_cleaned[available_cols]\n",
    "    print(f\"\\n1. Kolommen geselecteerd: {len(available_cols)} van {len(KEEP_COLUMNS)}\")\n",
    "    print(f\"   Behouden: {available_cols}\")\n",
    "\n",
    "    # 2. Vervang alle 'ntb' waarden met \"Nog niet bepaald\"\n",
    "    print(\"\\n2. Vervangen van 'ntb' waarden\")\n",
    "    before_counts = {}\n",
    "    for col in ['shortdescription', 'description', 'content', 'learningoutcomes']:\n",
    "        if col in df_cleaned.columns:\n",
    "            before_counts[col] = df_cleaned[col].str.lower().str.strip().eq('ntb').sum()\n",
    "    \n",
    "    df_cleaned = replace_ntb_values(df_cleaned)\n",
    "    \n",
    "    for col, count in before_counts.items():\n",
    "        print(f\"   {col}: {count} 'ntb' waarden vervangen\")\n",
    "    \n",
    "    # 2b. Vul lege module_tags met auto-gegenereerde tags\n",
    "    print(\"\\n2b. Auto-genereren van module tags voor modules zonder tags\")\n",
    "    before_empty = df_cleaned['module_tags'].apply(lambda x: len(x) == 0 if isinstance(x, list) else True).sum()\n",
    "    \n",
    "    df_cleaned = fill_empty_tags(df_cleaned)\n",
    "    \n",
    "    after_empty = df_cleaned['module_tags'].apply(lambda x: len(x) == 0 if isinstance(x, list) else True).sum()\n",
    "    print(f\"   {before_empty} modules zonder tags → {after_empty} na auto-generatie\")\n",
    "\n",
    "    # 3. Vul lege waarden in content velden\n",
    "    print(\"\\n3. Vullen van lege content velden\")\n",
    "    before_nulls = {\n",
    "        'shortdescription': df_cleaned['shortdescription'].isna().sum() + (df_cleaned['shortdescription'] == '').sum(),\n",
    "        'description': df_cleaned['description'].isna().sum() + (df_cleaned['description'] == '').sum(),\n",
    "        'content': df_cleaned['content'].isna().sum() + (df_cleaned['content'] == '').sum()\n",
    "    }\n",
    "    \n",
    "    df_cleaned = fill_missing_content(df_cleaned)\n",
    "    \n",
    "    for col, count in before_nulls.items():\n",
    "        after = df_cleaned[col].isna().sum() + (df_cleaned[col] == '').sum()\n",
    "        print(f\"   {col}: {count} lege velden → {after} na vullen\")\n",
    "\n",
    "    # 4. Vul learning outcomes met passende waarden\n",
    "    print(\"\\n4. Genereren van learning outcomes\")\n",
    "    before_lo = df_cleaned['learningoutcomes'].apply(\n",
    "        lambda x: pd.isna(x) or str(x).strip() == '' or str(x).lower() in ['ntb', 'nader te bepalen', 'nog niet bepaald']\n",
    "    ).sum()\n",
    "    \n",
    "    df_cleaned = fill_learning_outcomes(df_cleaned)\n",
    "    \n",
    "    after_lo = df_cleaned['learningoutcomes'].apply(\n",
    "        lambda x: pd.isna(x) or str(x).strip() == '' or str(x).lower() in ['ntb', 'nader te bepalen']\n",
    "    ).sum()\n",
    "    print(f\"   {before_lo} ontbrekende learning outcomes → {after_lo} na vullen\")\n",
    "\n",
    "    # 5. start_date naar geldige datetime\n",
    "    if \"start_date\" in df_cleaned.columns:\n",
    "        df_cleaned[\"start_date\"] = pd.to_datetime(df_cleaned[\"start_date\"], errors=\"coerce\")\n",
    "        invalid_dates = df_cleaned[\"start_date\"].isna().sum()\n",
    "        print(f\"\\n5. start_date geconverteerd naar datetime\")\n",
    "        print(f\"   Ongeldige datums naar NaT: {invalid_dates}\")\n",
    "\n",
    "    # 6. Duplicaten op id droppen\n",
    "    if \"id\" in df_cleaned.columns:\n",
    "        before = df_cleaned.shape[0]\n",
    "        duplicates = df_cleaned.duplicated(subset=[\"id\"]).sum()\n",
    "        df_cleaned = df_cleaned.drop_duplicates(subset=[\"id\"])\n",
    "        after = df_cleaned.shape[0]\n",
    "        print(f\"\\n6. Duplicaten op 'id'\")\n",
    "        print(f\"   Gevonden: {duplicates}, Rijen voor: {before}, na: {after}\")\n",
    "\n",
    "    # 7. Tekstvelden schoonmaken + lemmatizeren\n",
    "    print(\"\\n7. Tekstvelden normaliseren en lemmatiseren\")\n",
    "    for col in TEXT_COLS:\n",
    "        if col in df_cleaned.columns:\n",
    "            clean_col = f\"{col}_clean\"\n",
    "            print(f\"   - Verwerken: {col} -> {clean_col}\")\n",
    "            df_cleaned[clean_col] = df_cleaned[col].apply(normalize_text)\n",
    "\n",
    "    # 8. Globale missing value check\n",
    "    total_nulls = df_cleaned.isnull().sum().sum()\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINALE DATASET STATUS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Rijen: {df_cleaned.shape[0]}\")\n",
    "    print(f\"Kolommen: {df_cleaned.shape[1]}\")\n",
    "    print(f\"Totaal NULL waarden: {total_nulls}\")\n",
    "\n",
    "    # Optioneel: per kolom\n",
    "    print(\"\\nNULL waarden per kolom (alleen > 0):\")\n",
    "    nulls_per_col = df_cleaned.isnull().sum()\n",
    "    nulls_display = nulls_per_col[nulls_per_col > 0]\n",
    "    if len(nulls_display) > 0:\n",
    "        print(nulls_display)\n",
    "    else:\n",
    "        print(\"  Geen NULL waarden gevonden! \")\n",
    "\n",
    "    # Data quality check\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATA QUALITY VERIFICATIE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check voor 'ntb' waarden die nog over zijn\n",
    "    ntb_remaining = 0\n",
    "    for col in ['shortdescription', 'description', 'content', 'learningoutcomes']:\n",
    "        if col in df_cleaned.columns:\n",
    "            count = df_cleaned[col].str.lower().str.strip().eq('ntb').sum()\n",
    "            ntb_remaining += count\n",
    "            if count > 0:\n",
    "                print(f\"{col}: {count} 'ntb' waarden nog aanwezig!\")\n",
    "    \n",
    "    if ntb_remaining == 0:\n",
    "        print(\"Geen 'ntb' waarden meer aanwezig\")\n",
    "    \n",
    "    # Check voor lege velden\n",
    "    empty_critical = 0\n",
    "    for col in ['shortdescription', 'description', 'content']:\n",
    "        if col in df_cleaned.columns:\n",
    "            count = (df_cleaned[col].isna() | (df_cleaned[col].str.strip() == '')).sum()\n",
    "            empty_critical += count\n",
    "            if count > 0:\n",
    "                print(f\"{col}: {count} lege velden\")\n",
    "    \n",
    "    if empty_critical == 0:\n",
    "        print(\"Alle kritieke tekstvelden zijn gevuld\")\n",
    "    \n",
    "    # Check module_tags\n",
    "    if 'module_tags' in df_cleaned.columns:\n",
    "        empty_tags = df_cleaned['module_tags'].apply(lambda x: len(x) == 0 if isinstance(x, list) else True).sum()\n",
    "        total_tags = len(df_cleaned)\n",
    "        print(f\"\\nModule tags: {total_tags - empty_tags}/{total_tags} modules hebben tags\")\n",
    "        if empty_tags > 0:\n",
    "            print(f\"   {empty_tags} modules zonder tags (dit kan normaal zijn)\")\n",
    "\n",
    "    # Sample tonen\n",
    "    sample_cols = [c for c in [\"id\", \"name\", \"shortdescription\", \"learningoutcomes\"] if c in df_cleaned.columns]\n",
    "    print(\"\\nSample (eerste 3 rijen):\")\n",
    "    print(df_cleaned[sample_cols].head(3).to_string(max_colwidth=50))\n",
    "\n",
    "    # Opslaan\n",
    "    df_cleaned.to_csv(output_file, index=False)\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"OPGESLAGEN: {output_file}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_cleaned = clean_vkm_dataset(INPUT_FILE, OUTPUT_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
