{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26031b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA CLEANING PROCESS\n",
      "============================================================\n",
      "Origineel: 211 rijen, 20 kolommen\n",
      "\n",
      "1. Kleur-kolommen verwijderd: ['Rood', 'Groen', 'Blauw', 'Geel']\n",
      "   -> 16 kolommen resterend\n",
      "\n",
      "2. Vervang 'Ntb' waarden in tekstvelden\n",
      "   shortdescription: 20 NULL gevuld, 10 'Ntb' vervangen\n",
      "   description: 2 'Ntb' vervangen\n",
      "   content: 2 'Ntb' vervangen\n",
      "   learningoutcomes: 5 NULL gevuld, 26 'Ntb' vervangen\n",
      "\n",
      "4. start_date geconverteerd naar datetime\n",
      "   Ongeldige datums naar NaT: 0\n",
      "\n",
      "5. Duplicaten op 'id'\n",
      "   Gevonden: 0, Rijen voor: 211, na: 211\n",
      "\n",
      "6. Tekstvelden normaliseren en lemmatiseren\n",
      "   - Verwerken: shortdescription -> shortdescription_clean\n",
      "   - Verwerken: description -> description_clean\n",
      "   - Verwerken: content -> content_clean\n",
      "   - Verwerken: learningoutcomes -> learningoutcomes_clean\n",
      "\n",
      "============================================================\n",
      "FINALE DATASET STATUS\n",
      "============================================================\n",
      "Rijen: 211\n",
      "Kolommen: 20\n",
      "Totaal NULL waarden: 0\n",
      "\n",
      "NULL waarden per kolom (alleen > 0):\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Sample (eerste 3 rijen):\n",
      "    id                          name  \\\n",
      "0  159  Kennismaking met Psychologie   \n",
      "1  160   Learning and working abroad   \n",
      "2  161       Proactieve zorgplanning   \n",
      "\n",
      "                                    shortdescription  \\\n",
      "0  Brein, gedragsbeinvloeding, ontwikkelingspsych...   \n",
      "1  Internationaal, persoonlijke ontwikkeling, ver...   \n",
      "2     Proactieve zorgplanning, cocreatie, ziekenhuis   \n",
      "\n",
      "                              shortdescription_clean  \\\n",
      "0  brein gedragsbeinvloeding ontwikkelingspsychol...   \n",
      "1  internationaal persoonlijke ontwikkeling verpl...   \n",
      "2       proactieve zorgplanning cocreatie ziekenhuis   \n",
      "\n",
      "                                    learningoutcomes  \\\n",
      "0  A. Je beantwoordt vragen in een meerkeuze kenn...   \n",
      "1  De student toont professioneel gedrag conform ...   \n",
      "2  De student past pro actieve zorgplanning toe b...   \n",
      "\n",
      "                              learningoutcomes_clean  \n",
      "0  beantwoordt vragen meerkeuze kennistoets waari...  \n",
      "1  student toont professioneel gedrag conform ber...  \n",
      "2  student past pro actieve zorgplanning toe binn...  \n",
      "\n",
      "OPGESLAGEN: Uitgebreide_VKM_dataset_cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kloos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kloos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kloos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\kloos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "INPUT_FILE = \"Uitgebreide_VKM_dataset.csv\"\n",
    "OUTPUT_FILE = \"Uitgebreide_VKM_dataset_cleaned.csv\"\n",
    "\n",
    "# Kolommen die we als tekst willen schoonmaken\n",
    "TEXT_COLS = [\n",
    "    \"shortdescription\",\n",
    "    \"description\",\n",
    "    \"content\",\n",
    "    \"learningoutcomes\"\n",
    "]\n",
    "\n",
    "# Stopwoorden en lemmatizer\n",
    "# Kies 'english' of 'dutch' afhankelijk van je dataset\n",
    "STOP_LANG = \"dutch\"   # of \"english\"\n",
    "stop_words = set(stopwords.words(STOP_LANG))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_length(tokens, max_len=200):\n",
    "    \"\"\"Beperk tokens tot een vaste maximale lengte om scheefheid te verminderen.\"\"\"\n",
    "    return tokens[:max_len]\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Maak tekst schoon, lemmatiseer en normaliseer lengte.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Verwijder alles behalve letters, cijfers en spaties\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\sáéíóúàèìòùäëïöüâêîôûçñ]\", \" \", text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Filter op stopwoorden en korte tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "\n",
    "    # Lemmatiseer\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "    tokens = normalize_length(tokens, max_len=200)\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "\n",
    "def clean_vkm_dataset(input_file: str, output_file: str) -> pd.DataFrame:\n",
    "    # Data inladen\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA CLEANING PROCESS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Origineel: {df.shape[0]} rijen, {df.shape[1]} kolommen\")\n",
    "\n",
    "    df_cleaned = df.copy()\n",
    "\n",
    "    # 1. Verwijder de kleur-kolommen (Rood, Groen, Blauw, Geel) als ze bestaan\n",
    "    kleur_kolommen = [\"Rood\", \"Groen\", \"Blauw\", \"Geel\"]\n",
    "    bestaande_kleuren = [c for c in kleur_kolommen if c in df_cleaned.columns]\n",
    "    if bestaande_kleuren:\n",
    "        df_cleaned = df_cleaned.drop(columns=bestaande_kleuren)\n",
    "        print(f\"\\n1. Kleur-kolommen verwijderd: {bestaande_kleuren}\")\n",
    "    else:\n",
    "        print(\"\\n1. Geen kleur-kolommen gevonden om te verwijderen\")\n",
    "\n",
    "    print(f\"   -> {df_cleaned.shape[1]} kolommen resterend\")\n",
    "\n",
    "    # 2. Vervang \"Ntb\" waarden in alle tekstvelden\n",
    "    print(\"\\n2. Vervang 'Ntb' waarden in tekstvelden\")\n",
    "    ntb_replacements = {}\n",
    "    \n",
    "    # Voor shortdescription: gebruik description (eerste 200 chars)\n",
    "    if \"shortdescription\" in df_cleaned.columns and \"description\" in df_cleaned.columns:\n",
    "        # Vul NULL waarden\n",
    "        before_nulls = df_cleaned[\"shortdescription\"].isna().sum()\n",
    "        df_cleaned[\"shortdescription\"] = df_cleaned[\"shortdescription\"].fillna(\n",
    "            df_cleaned[\"description\"].astype(str).str[:200]\n",
    "        )\n",
    "        # Vervang \"Ntb\"\n",
    "        ntb_mask = df_cleaned[\"shortdescription\"].str.strip().str.lower() == \"ntb\"\n",
    "        df_cleaned.loc[ntb_mask, \"shortdescription\"] = df_cleaned.loc[ntb_mask, \"description\"].astype(str).str[:200]\n",
    "        ntb_replacements[\"shortdescription\"] = ntb_mask.sum()\n",
    "        print(f\"   shortdescription: {before_nulls} NULL gevuld, {ntb_mask.sum()} 'Ntb' vervangen\")\n",
    "    \n",
    "    # Voor description: gebruik content, of als die ook ntb is, gebruik shortdescription\n",
    "    if \"description\" in df_cleaned.columns:\n",
    "        ntb_mask = df_cleaned[\"description\"].str.strip().str.lower() == \"ntb\"\n",
    "        if \"content\" in df_cleaned.columns:\n",
    "            # Probeer eerst content\n",
    "            df_cleaned.loc[ntb_mask, \"description\"] = df_cleaned.loc[ntb_mask, \"content\"].astype(str).str[:500]\n",
    "            # Als content ook 'ntb' is, gebruik shortdescription\n",
    "            still_ntb = df_cleaned[\"description\"].str.strip().str.lower() == \"ntb\"\n",
    "            if \"shortdescription\" in df_cleaned.columns and still_ntb.sum() > 0:\n",
    "                df_cleaned.loc[still_ntb, \"description\"] = df_cleaned.loc[still_ntb, \"shortdescription\"].astype(str)\n",
    "        elif \"shortdescription\" in df_cleaned.columns:\n",
    "            df_cleaned.loc[ntb_mask, \"description\"] = df_cleaned.loc[ntb_mask, \"shortdescription\"].astype(str)\n",
    "        else:\n",
    "            df_cleaned.loc[ntb_mask, \"description\"] = \"Geen beschrijving beschikbaar\"\n",
    "        ntb_replacements[\"description\"] = ntb_mask.sum()\n",
    "        print(f\"   description: {ntb_mask.sum()} 'Ntb' vervangen\")\n",
    "    \n",
    "    # Voor content: gebruik description, of als die ook ntb is, gebruik shortdescription\n",
    "    if \"content\" in df_cleaned.columns:\n",
    "        ntb_mask = df_cleaned[\"content\"].str.strip().str.lower() == \"ntb\"\n",
    "        if \"description\" in df_cleaned.columns:\n",
    "            # Probeer eerst description\n",
    "            df_cleaned.loc[ntb_mask, \"content\"] = df_cleaned.loc[ntb_mask, \"description\"].astype(str)\n",
    "            # Als description ook 'ntb' is, gebruik shortdescription\n",
    "            still_ntb = df_cleaned[\"content\"].str.strip().str.lower() == \"ntb\"\n",
    "            if \"shortdescription\" in df_cleaned.columns and still_ntb.sum() > 0:\n",
    "                df_cleaned.loc[still_ntb, \"content\"] = df_cleaned.loc[still_ntb, \"shortdescription\"].astype(str)\n",
    "        elif \"shortdescription\" in df_cleaned.columns:\n",
    "            df_cleaned.loc[ntb_mask, \"content\"] = df_cleaned.loc[ntb_mask, \"shortdescription\"].astype(str)\n",
    "        else:\n",
    "            df_cleaned.loc[ntb_mask, \"content\"] = \"Geen content beschikbaar\"\n",
    "        ntb_replacements[\"content\"] = ntb_mask.sum()\n",
    "        print(f\"   content: {ntb_mask.sum()} 'Ntb' vervangen\")\n",
    "    \n",
    "    # Voor learningoutcomes: gebruik vaste tekst\n",
    "    if \"learningoutcomes\" in df_cleaned.columns:\n",
    "        before_nulls = df_cleaned[\"learningoutcomes\"].isna().sum()\n",
    "        df_cleaned[\"learningoutcomes\"] = df_cleaned[\"learningoutcomes\"].fillna(\"Nog niet bepaald\")\n",
    "        ntb_mask = df_cleaned[\"learningoutcomes\"].str.strip().str.lower() == \"ntb\"\n",
    "        df_cleaned.loc[ntb_mask, \"learningoutcomes\"] = \"Nog niet bepaald\"\n",
    "        ntb_replacements[\"learningoutcomes\"] = ntb_mask.sum()\n",
    "        print(f\"   learningoutcomes: {before_nulls} NULL gevuld, {ntb_mask.sum()} 'Ntb' vervangen\")\n",
    "\n",
    "    # 3. Vul overige NULL waarden in shortdescription\n",
    "    if \"shortdescription\" in df_cleaned.columns and \"description\" in df_cleaned.columns:\n",
    "        null_mask = df_cleaned[\"shortdescription\"].isna()\n",
    "        if null_mask.sum() > 0:\n",
    "            df_cleaned.loc[null_mask, \"shortdescription\"] = df_cleaned.loc[null_mask, \"description\"].astype(str).str[:200]\n",
    "            print(f\"\\n3. Resterende NULL waarden in shortdescription aangevuld: {null_mask.sum()}\")\n",
    "    else:\n",
    "        print(\"\\n3. Geen extra NULL waarden om aan te vullen\")\n",
    "\n",
    "    # 4. start_date naar geldige datetime\n",
    "    if \"start_date\" in df_cleaned.columns:\n",
    "        df_cleaned[\"start_date\"] = pd.to_datetime(df_cleaned[\"start_date\"], errors=\"coerce\")\n",
    "        invalid_dates = df_cleaned[\"start_date\"].isna().sum()\n",
    "        print(f\"\\n4. start_date geconverteerd naar datetime\")\n",
    "        print(f\"   Ongeldige datums naar NaT: {invalid_dates}\")\n",
    "    else:\n",
    "        print(\"\\n4. Kolom start_date ontbreekt, stap overgeslagen\")\n",
    "\n",
    "    # 5. Duplicaten op id droppen\n",
    "    if \"id\" in df_cleaned.columns:\n",
    "        before = df_cleaned.shape[0]\n",
    "        duplicates = df_cleaned.duplicated(subset=[\"id\"]).sum()\n",
    "        df_cleaned = df_cleaned.drop_duplicates(subset=[\"id\"])\n",
    "        after = df_cleaned.shape[0]\n",
    "        print(f\"\\n5. Duplicaten op 'id'\")\n",
    "        print(f\"   Gevonden: {duplicates}, Rijen voor: {before}, na: {after}\")\n",
    "    else:\n",
    "        print(\"\\n5. Kolom 'id' ontbreekt, duplicaten-check overgeslagen\")\n",
    "\n",
    "    # 6. Tekstvelden schoonmaken + lemmatizeren\n",
    "    print(\"\\n6. Tekstvelden normaliseren en lemmatiseren\")\n",
    "    for col in TEXT_COLS:\n",
    "        if col in df_cleaned.columns:\n",
    "            clean_col = f\"{col}_clean\"\n",
    "            print(f\"   - Verwerken: {col} -> {clean_col}\")\n",
    "            df_cleaned[clean_col] = df_cleaned[col].apply(normalize_text)\n",
    "        else:\n",
    "            print(f\"   - Kolom {col} niet gevonden, overgeslagen\")\n",
    "\n",
    "    # 7. Globale missing value check\n",
    "    total_nulls = df_cleaned.isnull().sum().sum()\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINALE DATASET STATUS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Rijen: {df_cleaned.shape[0]}\")\n",
    "    print(f\"Kolommen: {df_cleaned.shape[1]}\")\n",
    "    print(f\"Totaal NULL waarden: {total_nulls}\")\n",
    "\n",
    "    # Optioneel: per kolom\n",
    "    print(\"\\nNULL waarden per kolom (alleen > 0):\")\n",
    "    nulls_per_col = df_cleaned.isnull().sum()\n",
    "    print(nulls_per_col[nulls_per_col > 0])\n",
    "\n",
    "    # Sample tonen\n",
    "    sample_cols = [c for c in [\"id\", \"name\", \"shortdescription\", \"shortdescription_clean\", \"learningoutcomes\", \"learningoutcomes_clean\"] if c in df_cleaned.columns]\n",
    "    print(\"\\nSample (eerste 3 rijen):\")\n",
    "    print(df_cleaned[sample_cols].head(3))\n",
    "\n",
    "    # Opslaan\n",
    "    df_cleaned.to_csv(output_file, index=False)\n",
    "    print(f\"\\nOPGESLAGEN: {output_file}\")\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_cleaned = clean_vkm_dataset(INPUT_FILE, OUTPUT_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
